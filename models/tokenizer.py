import torch
import regex as re
from typing import List, Dict, Union
import html


class CLIPTokenizer:
    """
    CLIPç”¨ã®BPE (Byte Pair Encoding) ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    
    å‡¦ç†ã®æµã‚Œ:
    1. Text Cleaning: ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ãƒ»æ­£è¦åŒ–
    2. BPE Encoding: ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½ã¸ã®åˆ†å‰²
    3. Token Mapping: èªå½™è¾æ›¸ã‚’ä½¿ã£ã¦IDã«å¤‰æ›
    4. Special Tokens: SOT/EOT/PADãƒˆãƒ¼ã‚¯ãƒ³ã®è¿½åŠ 
    5. Sequence Padding: å›ºå®šé•·ã¸ã®èª¿æ•´
    """
    
    def __init__(self, vocab_file: str = None,bpe_merge_file: str = None, max_length: int = 77):
        """
        Args:
            vocab_file: BPEèªå½™ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            max_length: æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· (CLIPã§ã¯77ãŒæ¨™æº–)
        """
        self.max_length = max_length
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ•ãƒ©ã‚°
        self.verbose = False  # å…¨ä½“ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ•ãƒ©ã‚°
        self.clean_text_verbose = False  # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã®ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ•ãƒ©ã‚°
        self.bpe_verbose = False  # BPEã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã®ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ•ãƒ©ã‚°
        
        # Step 1: Special tokensã‚’å®šç¾©
        # CLIPã§ä½¿ç”¨ã•ã‚Œã‚‹ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        self.pad_token = "<|pad|>"      # ID: 0
        self.sot_token = "<|startoftext|>"  # ID: 1  
        self.eot_token = "<|endoftext|>"    # ID: vocab_size-1
        self.unk_token = "<|unk|>"      # æœªçŸ¥èªç”¨
        
        # Step 2: BPEèªå½™è¾æ›¸ã‚’èª­ã¿è¾¼ã¿
        # 
        # _file ã‹ã‚‰ token -> ID ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰
        if vocab_file is None:
            self.vocab_file = "/home/ryuichi/face_editing/RelatedWork/clip_scratch/models/vocab/vocab.json"
        self.vocab = self._load_vocab(vocab_file)
        self.vocab_size = len(self.vocab)
        
        # Step 3: é€†å¼•ãè¾æ›¸ã‚’ä½œæˆ (ID -> token)
        # ãƒ‡ã‚³ãƒ¼ãƒ‰æ™‚ã«ä½¿ç”¨ï¼ˆKey:IDâ‡’ID:Keyã«å¤‰æ›ã—ã¦é€†å¼•ãè¾æ›¸ã‚’ä½œæˆï¼‰
        self.id_to_token = {v: k for k, v in self.vocab.items()}
        
        # Step 4: BPEãƒãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«ã‚’èª­ã¿è¾¼ã¿
        # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ã®ãŸã‚ã®ãƒ«ãƒ¼ãƒ«
        if bpe_merge_file is None:
            bpe_merge_file = "/home/ryuichi/face_editing/RelatedWork/clip_scratch/models/vocab/bpe_simple_vocab_16e6.txt"
        self.bpe_merges = self._load_bpe_merges(bpe_merge_file)
        
        # Step 5: ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ç”¨ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰
        # regexãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨ã§Unicodeæ–‡å­—ã‚¯ãƒ©ã‚¹ã«å¯¾å¿œ
        # æ—¥æœ¬èª: ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»é•·éŸ³è¨˜å·ã‚’ä¸€ç·’ã«å‡¦ç†
        self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d|[\p{Hiragana}ãƒ¼]+|[\p{Katakana}ãƒ¼]+|[\p{Han}]+|\b\p{L}+\b| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")
        
    def _load_vocab(self, vocab_file: str) -> Dict[str, int]:
        """
        CLIPèªå½™è¾æ›¸ï¼ˆJSONï¼‰ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            vocab_file: èªå½™ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆä½¿ç”¨ã—ãªã„ã€å›ºå®šãƒ‘ã‚¹ï¼‰
            
        Returns:
            vocab: token -> ID ã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸
        """
        import json
        
        # CLIPå…¬å¼ã®èªå½™è¾æ›¸JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        vocab_json_path = "/home/ryuichi/face_editing/RelatedWork/clip_scratch/models/vocab/vocab.json"
        
        with open(vocab_json_path, 'r', encoding='utf-8') as f:
            vocab = json.load(f)
        
        print(f"Loaded vocabulary with {len(vocab)} tokens")
        return vocab

    def _load_bpe_merges(self, bpe_merge_file: str) -> Dict[tuple, int]:
        """
        BPEãƒãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            bpe_merge_file: BPEãƒãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            merges: {(token1, token2): priority} ã®ãƒãƒ¼ã‚¸ãƒšã‚¢è¾æ›¸
        """
        if bpe_merge_file is None:
            bpe_merge_file = "/home/ryuichi/face_editing/RelatedWork/clip_scratch/models/vocab/bpe_simple_vocab_16e6.txt"
        if self.verbose:
            print(f"Loading BPE merges from: {bpe_merge_file}")
        
        merges = {}
        with open(bpe_merge_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
            # 1è¡Œç›®ã¯ãƒ˜ãƒƒãƒ€ãƒ¼ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
            for i, line in enumerate(lines[1:]):
                line = line.strip()#strip()  # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
                if line and not line.startswith('#'):
                    parts = line.split()#split()  # ç©ºç™½ã§åˆ†å‰²
                    if len(parts) == 2:
                        # è¡Œç•ªå·ã‚’å„ªå…ˆåº¦ã¨ã—ã¦ä½¿ç”¨ï¼ˆå°ã•ã„ã»ã©é«˜å„ªå…ˆåº¦ï¼‰
                        merges[(parts[0], parts[1])] = i
        
        print(f"Loaded {len(merges)} BPE merge rules")
        return merges
    
    def _clean_text(self, text: str) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ãƒ»æ­£è¦åŒ–
        
        Args:
            text: ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            cleaned_text: æ­£è¦åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        # Step 1: åŸºæœ¬çš„ãªæ­£è¦åŒ–
        # å°æ–‡å­—åŒ–(lower)ã€ä½™åˆ†ãªç©ºç™½ã®é™¤å»ï¼ˆstripï¼‰
        if self.clean_text_verbose:
            print(f"{'before strip and lower':<25}:{text}")
        
        text = text.strip().lower()
        
        if self.clean_text_verbose:
            print(f"{'after strip and lower':<25}:{text}")
        
        # Step 2: ç‰¹æ®Šæ–‡å­—ã®å‡¦ç†
        # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ã€å¥èª­ç‚¹ã®æ­£è¦åŒ–
        if self.clean_text_verbose:
            print(f"{'before html unescape':<25}:{text}")
        
        text = html.unescape(text)
        
        if self.clean_text_verbose:
            print(f"{'after html unescape':<25}:{text}")
        
        # Step 3: Unicodeæ­£è¦åŒ–
        import unicodedata
        if self.clean_text_verbose:
            print(f"{'before unicode normalize':<25}:{text}")
        
        # Unicodeæ­£è¦åŒ–ã‚’è¡Œã„ã€NFKCå½¢å¼ã«å¤‰æ›
        text = unicodedata.normalize('NFKC', text)
        
        if self.clean_text_verbose:
            print(f"{'after unicode normalize':<25}:{text}")
        
        return text
  
    
    def _bpe_encode(self, text: str) -> List[str]:
        """
        BPE (Byte Pair Encoding) ã§ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²
        
        Args:
            text: å‰å‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            bpe_tokens: BPEãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        # Step 1: æ­£è¦è¡¨ç¾ã§ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²(ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹éƒ¨åˆ†ã‚’æŠ½å‡º)
        # å˜èªã€æ•°å­—ã€å¥èª­ç‚¹ã‚’é©åˆ‡ã«åˆ†é›¢
        if self.bpe_verbose:
            print(f"{'before regex split':<25}:{text}")
        
        tokens = re.findall(self.pat, text)
        
        if self.bpe_verbose:
            print(f"{'after regex split':<25}:{tokens}")
        
        # Step 2: å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’BPEã§ã•ã‚‰ã«åˆ†å‰²
        bpe_tokens = []
        for token in tokens:
            # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã«åˆ†è§£(this ->[ 't', 'h', 'i', 's'])
            if self.bpe_verbose:
                print(f"{'token before BPE':<25}:{token}")
            
            word = list(token)
            
            if self.bpe_verbose:
                print(f"{'word before BPE':<25}:{word}")
          
            # BPEãƒãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨
            while len(word) >= 2:
                # ã™ã¹ã¦ã®éš£æ¥ãƒšã‚¢ã‚’å–å¾—([t,h, i, s] -> [(t,h), (h,i), (i,s)]
                pairs = [(word[i], word[i+1]) for i in range(len(word)-1)]
                
                # if self.bpe_verbose:
                #     print(f"{'éš£æ¥ãƒšã‚¢':<25}:{pairs}")
                
                # BPEãƒãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ã„ã¦æœ€ã‚‚é »å‡ºã™ã‚‹ãƒšã‚¢ã‚’å–å¾—
                  # (t,h)->0, (h,i)->1, (i,s)->2ã¿ãŸã„ãªæ„Ÿã˜â†’(t,h)
                bigram = min(pairs, key=lambda pair: self.bpe_merges.get(pair, float('inf')))
                
                if self.bpe_verbose:
                    print(f"{'æœ€é »å‡ºãƒšã‚¢':<25}:{bigram}")
                
                #ã‚‚ã—ã‚‚æœ€é »å‡ºãƒšã‚¢ãŒèªå½™è¾æ›¸ã«å­˜åœ¨ã—ãªã„å ´åˆ
                if bigram not in self.bpe_merges:
                    break        
                # ãƒãƒ¼ã‚¸ã‚’å®Ÿè¡Œ
                new_word = []
                i = 0
                while i < len(word):
                    #æœ€çµ‚æ–‡å­—ã§ãªã„å ´åˆã‹ã¤word[i]ã¨word[i+1]ãŒæœ€é »å‡ºãƒšã‚¢ãªã‚‰ã°
                    if i < len(word) - 1 and (word[i], word[i+1]) == bigram:
                        # èªã©ã†ã—ã‚’çµåˆ
                        new_word.append(word[i] + word[i+1])
                        i += 2
                    #æœ€çµ‚æ–‡å­—ã¾ãŸã¯word[i]ã¨word[i+1]ãŒæœ€é »å‡ºãƒšã‚¢ã§ãªã„å ´åˆ
                    else:
                        # ãã®ã¾ã¾è¿½åŠ 
                        new_word.append(word[i])
                        i += 1
                word = new_word
            bpe_tokens.extend(word)
   
        if self.bpe_verbose:
            print(f"{'final bpe_tokens':<25}:{bpe_tokens}")
        return bpe_tokens
      
    
    def encode(self, text: Union[str, List[str]]) -> torch.Tensor:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ãƒªã‚¹ãƒˆï¼‰
            
        Returns:
            token_ids: ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ†ãƒ³ã‚½ãƒ« [batch_size, max_length]
        """
        # å˜ä¸€æ–‡å­—åˆ—ã®å ´åˆã¯ãƒªã‚¹ãƒˆã«å¤‰æ›
        if isinstance(text, str):
            text = [text]
        
        batch_tokens = []
        for single_text in text:
            if self.verbose:
                print(f"{'looping text':<25}:{single_text}")
            # Step 1: ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
            cleaned_text = self._clean_text(single_text) 
            if self.verbose:
                print(f"{'cleaned text':<25}:{cleaned_text}")
            # Step 2: BPEã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            bpe_tokens = self._bpe_encode(cleaned_text)

            # Step 3: èªå½™è¾æ›¸ã§IDã«å¤‰æ›
            token_ids = [self.vocab.get(self.sot_token, 1)]  # SOTãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…ˆé ­ã«
        
            for token in bpe_tokens:
                token_id = self.vocab.get(token, self.vocab.get(self.unk_token, 0))
                token_ids.append(token_id)
                if self.verbose:
                    print(f"{'token':<25}:{token}, id: {token_id}")
            # Step 4: EOTãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
            token_ids.append(self.vocab.get(self.eot_token, self.vocab_size - 1))
            if self.verbose:
                print(f"{'step3~4 token_ids':<25}:{token_ids}")
            # Step 5: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°(ã‚¼ãƒ­åŸ‹ã‚)ã¾ãŸã¯ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆ(ã¯ã¿å‡ºãŸå ´åˆã¯åˆ‡ã‚Šæ¨ã¦))
            if len(token_ids) > self.max_length:
                # é•·ã™ãã‚‹å ´åˆã¯ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆï¼ˆEOTã¯ä¿æŒï¼‰
                token_ids = token_ids[:self.max_length-1] + [token_ids[-1]]
            else:
                # çŸ­ã„å ´åˆã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                token_ids.extend([0] * (self.max_length - len(token_ids)))
            
            batch_tokens.append(token_ids)
        
        return torch.tensor(batch_tokens, dtype=torch.long)
    def decode(self, token_ids: torch.Tensor) -> List[str]:
        """
        ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        
        Args:
            token_ids: ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ†ãƒ³ã‚½ãƒ« [batch_size, seq_len]
            
        Returns:
            texts: ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        texts = []
        for sequence in token_ids:
            tokens = []
            # token id åˆ—ã‚’å‡¦ç†ã—ã¦ã€idã‹ã‚‰text ã«å¤‰æ›
            for token_id in sequence:
                # item()ã§ã‚¹ã‚«ãƒ©ãƒ¼å€¤ã«å¤‰æ›
                token_id = token_id.item()     
                # ç‰¹æ®ŠIDã®å‡¦ç†
                if token_id == 0:  # PADãƒˆãƒ¼ã‚¯ãƒ³ or æœªçŸ¥èªãƒˆãƒ¼ã‚¯ãƒ³ã®å ´åˆ
                    continue
                # SOTãƒˆãƒ¼ã‚¯ãƒ³ã¯ç„¡è¦–ï¼ˆå…ˆé ­ã«ã‚ã‚‹ãŸã‚ï¼‰
                elif token_id == self.vocab.get(self.sot_token, 1):  # SOT
                    continue
                # EOTãƒˆãƒ¼ã‚¯ãƒ³ã¯çµ‚äº†
                elif token_id == self.vocab.get(self.eot_token, self.vocab_size - 1):  # EOT
                    break
           
                else:
                    #idã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›
                    token = self.id_to_token.get(token_id, self.unk_token)
                    tokens.append(token)
            
            # BPEãƒˆãƒ¼ã‚¯ãƒ³ã‚’çµåˆã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«å¾©å…ƒ
            if self.verbose:
                print(f"{'tokens before join':<25}:{tokens}")
            text = ''.join(tokens).replace('</w>', ' ').strip()
            if self.verbose:
                print(f"{'text after join':<25}:{text}")
            texts.append(text)
        
        return texts
        # pass
    
    def __len__(self) -> int:
        """èªå½™ã‚µã‚¤ã‚ºã‚’è¿”ã™"""
        # return self.vocab_size
        pass
    def test_clip_tokenizer(self):
        """
        CLIPãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åŸºæœ¬çš„ãªãƒ†ã‚¹ãƒˆ
        """
        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ•ãƒ©ã‚°ã‚’æœ‰åŠ¹åŒ–
        self.verbose = True
        self.clean_text_verbose = True
        self.bpe_verbose = True
        # ãƒ†ã‚¹ãƒˆç”¨ã®ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆ
        test_text_list = ["roulstonç§ã®åå‰ã¯å®®å†…ç«œä¸€ã§ã™.ç¬‘é¡”ãŒã¨ã¦ã‚‚ç´ æ•µã§ãƒ¦ãƒ¼ãƒ¢ã‚¢ã«æº¢ã‚Œã¦ã„ã¾ã™ã€‚",
                          "ï¼‘ï¼’ï¼“ï¼”56789ã¯ï¼‘ï¼é€²æ•°ã§ã™ã€10é€²æ•°ã§ã™ï¿¥",
                          "This is a maple test sentence with some punctuation! ğŸ˜Š",
                          ]
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        token_ids = self.encode(test_text_list)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        decoded_text = self.decode(token_ids)
        
        print("Original Text:", test_text_list)
        print("Token IDs:", token_ids)
        print("Decoded Text:", decoded_text)
        
if __name__ == "__main__":
    tokenizer = CLIPTokenizer()
    tokenizer.test_clip_tokenizer()